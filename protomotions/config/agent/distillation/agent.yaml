# @package _global_

num_envs: 512
agent:
  _target_: protomotions.agents.distillation.agent.DistillationAgent
  _recursive_: False
  config:
    # Setup lightweight student model
    model:
      _target_: protomotions.agents.distillation.model.LightweightStudentModel
      _recursive_: False
      config:
        actor:
          _target_: protomotions.agents.distillation.model.LightweightActor
          _recursive_: False
          num_out: ${robot.number_of_actions}
          config:
            actor_logstd: -2.9
            mu_model:
              _target_: protomotions.agents.distillation.model.CompactTransformer
              _recursive_: False
              num_out: ${robot.number_of_actions}
              config:
                transformer_token_size: ${.latent_dim}
                latent_dim: 256  # Reduced from 512
                ff_size: 512     # Reduced from 1024
                num_layers: 2    # Reduced from 4
                num_heads: 4
                dropout: 0.1
                
                input_models:
                  self_obs:
                    _target_: protomotions.agents.common.mlp.MLP_WithNorm
                    _recursive_: False
                    num_in: ${robot.self_obs_size}
                    num_out: ${...transformer_token_size}
                    config:
                      obs_key: self_obs
                      normalize_obs: True
                      norm_clamp_value: 5
                      layers:
                        - units: 128  # Reduced from 256
                          activation: relu
                          use_layer_norm: false
                        - units: 128  # Reduced from 256
                          activation: relu
                          use_layer_norm: false
                  
                  # Add path following input
                  path:
                    _target_: protomotions.agents.common.mlp.MLP_WithNorm
                    _recursive_: False
                    num_in: ${env.config.path_follower_params.path_obs_size}
                    num_out: ${...transformer_token_size}
                    config:
                      obs_key: path
                      normalize_obs: True
                      norm_clamp_value: 5
                      layers:
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                  
                  # Add goal input
                  goal:
                    _target_: protomotions.agents.common.mlp.MLP_WithNorm
                    _recursive_: False
                    num_in: ${env.config.goal_params.obs_size}
                    num_out: ${...transformer_token_size}
                    config:
                      obs_key: goal
                      normalize_obs: True
                      norm_clamp_value: 5
                      layers:
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                  
                  # Add steering input
                  steering:
                    _target_: protomotions.agents.common.mlp.MLP_WithNorm
                    _recursive_: False
                    num_in: ${env.config.steering_params.obs_size}
                    num_out: ${...transformer_token_size}
                    config:
                      obs_key: steering
                      normalize_obs: True
                      norm_clamp_value: 5
                      layers:
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                        - units: 64
                          activation: relu
                          use_layer_norm: false
                
                output_model:
                  _target_: protomotions.agents.common.mlp.MLP
                  _recursive_: False
                  num_in: ${..transformer_token_size}
                  num_out: ${robot.number_of_actions}
                  config:
                    layers:
                      - units: 256  # Reduced from 512
                        activation: relu
                        use_layer_norm: false
                      - units: 128  # Reduced from 256
                        activation: relu
                        use_layer_norm: false
        
        critic:
          _target_: protomotions.agents.common.mlp.MultiHeadedMLP
          _recursive_: False
          num_out: 1
          config:
            input_models:
              self_obs:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${robot.self_obs_size}
                num_out: ${.num_in}
                config:
                  obs_key: self_obs
                  normalize_obs: True
                  norm_clamp_value: 5
              path:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${env.config.path_follower_params.path_obs_size}
                num_out: ${.num_in}
                config:
                  obs_key: path
                  normalize_obs: True
                  norm_clamp_value: 5
              goal:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${env.config.goal_params.obs_size}
                num_out: ${.num_in}
                config:
                  obs_key: goal
                  normalize_obs: True
                  norm_clamp_value: 5
              steering:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${env.config.steering_params.obs_size}
                num_out: ${.num_in}
                config:
                  obs_key: steering
                  normalize_obs: True
                  norm_clamp_value: 5
            trunk:
              _target_: protomotions.agents.common.mlp.MLP
              _recursive_: False
              num_in: ${eval:${robot.self_obs_size}+${env.config.path_follower_params.path_obs_size}+${env.config.goal_params.obs_size}+${env.config.steering_params.obs_size}}
              num_out: 1
              config:
                layers:
                  - units: 256  # Reduced from 512
                    activation: relu
                    use_layer_norm: false
                  - units: 128  # Reduced from 256
                    activation: relu
                    use_layer_norm: false
        
        optimizer:
          _target_: torch.optim.Adam
          lr: 3e-4
    
    # Teacher models configuration
    teacher_models:
      path_follower:
        checkpoint_path: "results/smpl_path_follower"
        model_type: "ppo"
        weight: 1.0
      
      goal_directed:
        checkpoint_path: "results/smpl_goal_directed"
        model_type: "ppo"
        weight: 1.0
      
      steering:
        checkpoint_path: "results/smpl_steering"
        model_type: "ppo"
        weight: 1.0
      
      masked_mimic:
        checkpoint_path: "results/smpl_masked_mimic"
        model_type: "masked_mimic"
        weight: 1.0
    
    # Distillation parameters
    distillation_weight: 1.0      # Weight for action distillation loss
    kl_weight: 0.1                # Weight for KL divergence loss
    value_distillation_weight: 0.5 # Weight for value distillation loss
    
    # PPO parameters
    num_steps: 32
    tau: 0.95
    gamma: 0.99
    e_clip: 0.2
    clip_critic_loss: True
    gradient_clip_val: 0.5
    fail_on_bad_grads: False
    check_grad_mag: True
    bounds_loss_coef: 10
    normalize_values: True
    normalized_val_clamp_value: 5
    normalize_advantage: True
    batch_size: 2048
    task_reward_w: 1.0
    num_mini_epochs: 1
    max_eval_steps: null
    eval_metrics_every: 500
    eval_metric_keys: ["cartesian_err", "gt_err", "path_following_err", "goal_reaching_err", "steering_err"]
    training_early_termination: null
    num_games: null
    manual_save_every: 10
    max_epochs: ${eval:${training_max_steps}//${ngpu}//${num_envs}//${.num_steps}}
    
    # Extra inputs for all capabilities
    extra_inputs:
      path: true
      goal: true
      steering: true 